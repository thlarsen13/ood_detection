{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow_probability.python import math as tfp_math\n",
    "from tensorflow_probability.python.internal import dtype_util\n",
    "from tensorflow_probability.python.internal import prefer_static as ps\n",
    "from tensorflow_probability.python.internal import tensorshape_util\n",
    "from tensorflow_probability.python.stats import quantiles as quantiles_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_calibration_bin_statistics(\n",
    "    num_bins, logits=None, labels_true=None):\n",
    "    \"\"\" Compute binning statistics required for calibration measures.\n",
    "  Args:\n",
    "    num_bins: int, number of probability bins, e.g. 10.\n",
    "    logits: Tensor, (n,nlabels), with logits for n instances and nlabels.\n",
    "    labels_true: Tensor, (n,), with tf.int32 or tf.int64 elements containing\n",
    "      ground truth class labels in the range [0,nlabels].\n",
    "  Returns:\n",
    "    bz: Tensor, shape (2,num_bins), tf.int32, counts of incorrect (row 0) and\n",
    "      correct (row 1) predictions in each of the `num_bins` probability bins.\n",
    "    pmean_observed: Tensor, shape (num_bins,), tf.float32, the mean predictive\n",
    "      probabilities in each probability bin.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We take the label with the maximum probability\n",
    "    # decision.  This corresponds to the optimal expected minimum loss decision\n",
    "    # under 0/1 loss.\n",
    "#         pred_y = tf.argmax(logits, axis=1, output_type=labels_true.dtype)\n",
    "    pred_y = tf.argmax(logits, axis=1)\n",
    "\n",
    "\n",
    "    correct = tf.cast(tf.equal(pred_y, labels_true), tf.int32)\n",
    "\n",
    "    # Collect predicted probabilities of decisions\n",
    "    pred = tf.nn.softmax(logits, axis=1)\n",
    "    prob_y = tf.gather(\n",
    "      pred, pred_y[:, tf.newaxis], batch_dims=1)  # p(pred_y | x)\n",
    "    prob_y = tf.reshape(prob_y, (ps.size(prob_y),))\n",
    "\n",
    "    # Compute b/z histogram statistics:\n",
    "    # bz[0,bin] contains counts of incorrect predictions in the probability bin.\n",
    "    # bz[1,bin] contains counts of correct predictions in the probability bin.\n",
    "    bins = tf.histogram_fixed_width_bins(prob_y, [0.0, 1.0], nbins=num_bins)\n",
    "    event_bin_counts = tf.math.bincount(\n",
    "      correct * num_bins + bins,\n",
    "      minlength=2 * num_bins,\n",
    "      maxlength=2 * num_bins)\n",
    "    event_bin_counts = tf.reshape(event_bin_counts, (2, num_bins))\n",
    "\n",
    "    # Compute mean predicted probability value in each of the `num_bins` bins\n",
    "    pmean_observed = tf.math.unsorted_segment_sum(prob_y, bins, num_bins)\n",
    "    tiny = np.finfo(dtype_util.as_numpy_dtype(logits.dtype)).tiny\n",
    "    pmean_observed = pmean_observed / (\n",
    "      tf.cast(tf.reduce_sum(event_bin_counts, axis=0), logits.dtype) + tiny)\n",
    "\n",
    "    return event_bin_counts, pmean_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeESE(num_bins, logits=None, labels_true=None): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_bins: int, number of probability bins to compute oberved likelyhoods of correctness, e.g. 10.\n",
    "        logits: Tensor, (n,nlabels), with logits for n instances and nlabels.\n",
    "        labels_true: Tensor, (n,), with tf.int32 or tf.int64 elements containing\n",
    "          ground truth class labels in the range [0,nlabels].\n",
    "    Returns:\n",
    "        ece: Tensor, scalar, tf.float32.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('expected_calibration_error'):\n",
    "        logits = tf.convert_to_tensor(logits)\n",
    "        labels_true = tf.convert_to_tensor(labels_true)\n",
    "        labels_true = tf.cast(labels_true, dtype=tf.int64)\n",
    "\n",
    "        # Compute empirical counts over the events defined by the sets\n",
    "        # {incorrect,correct}x{0,1,..,num_bins-1}, as well as the empirical averages\n",
    "        # of predicted probabilities in each probability bin.\n",
    "        event_bin_counts, pmean_observed = _compute_calibration_bin_statistics(\n",
    "            num_bins, logits=logits, labels_true=labels_true)\n",
    "\n",
    "        # Compute the marginal probability of observing a probability bin.\n",
    "        event_bin_counts = tf.cast(event_bin_counts, tf.float32)\n",
    "        bin_n = tf.reduce_sum(event_bin_counts, axis=0)\n",
    "        pbins = bin_n / tf.reduce_sum(bin_n)  # Compute the marginal bin probability\n",
    "\n",
    "        # Compute the marginal probability of making a correct decision given an\n",
    "        # observed probability bin.\n",
    "        tiny = np.finfo(np.float32).tiny\n",
    "        pcorrect = event_bin_counts[1, :] / (bin_n + tiny)\n",
    "\n",
    "        # Compute the ESE statistic which is supposed to be analagous to brier score\n",
    "        ese = tf.reduce_sum(pbins * tf.square(pcorrect - pmean_observed))\n",
    "    return ese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "# cal_optimizer = keras.optimizers.SGD(learning_rate=1e-8)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "cce_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "num_bins=10\n",
    "gamma = 10**0\n",
    "def loss_fn(y_batch_train, logits, verbose=False): \n",
    "    cce = cce_loss_fn(y_batch_train, logits) \n",
    "    ese = tf.multiply(gamma, computeESE(num_bins, logits=logits, labels_true=y_batch_train))\n",
    "    if verbose:\n",
    "        print(f\"Training cce, ese, loss (for one batch): {cce:.4f}, {ese:.4f}, {cce+ese:.4f}\")\n",
    "\n",
    "#     return tf.add(cce, ese)\n",
    "    return ese\n",
    "\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training cce, ese, loss (for one batch): 110.4293, 0.2649, 110.6942\n",
      "Training cce, ese, loss (for one batch): 455.8655, 0.7932, 456.6587\n",
      "Training cce, ese, loss (for one batch): 218.4542, 0.5847, 219.0389\n",
      "Training cce, ese, loss (for one batch): 243.1186, 0.6314, 243.7499\n",
      "\n",
      "Start of epoch 1\n",
      "Training cce, ese, loss (for one batch): 384.8776, 0.7086, 385.5862\n",
      "Training cce, ese, loss (for one batch): 219.6416, 0.5728, 220.2143\n",
      "Training cce, ese, loss (for one batch): 649.2415, 0.7385, 649.9800\n",
      "Training cce, ese, loss (for one batch): 600.5511, 0.7932, 601.3444\n",
      "\n",
      "Start of epoch 2\n",
      "Training cce, ese, loss (for one batch): 621.3594, 0.8213, 622.1807\n",
      "Training cce, ese, loss (for one batch): 363.5745, 0.5862, 364.1607\n",
      "Training cce, ese, loss (for one batch): 493.9208, 0.6350, 494.5558\n",
      "Training cce, ese, loss (for one batch): 567.8087, 0.6602, 568.4688\n",
      "\n",
      "Start of epoch 3\n",
      "Training cce, ese, loss (for one batch): 394.3954, 0.6602, 395.0555\n",
      "Training cce, ese, loss (for one batch): 436.1915, 0.6602, 436.8516\n",
      "Training cce, ese, loss (for one batch): 453.2186, 0.6335, 453.8521\n",
      "Training cce, ese, loss (for one batch): 936.0533, 0.9084, 936.9617\n",
      "\n",
      "Start of epoch 4\n",
      "Training cce, ese, loss (for one batch): 1053.4120, 0.9084, 1054.3204\n",
      "Training cce, ese, loss (for one batch): 836.2468, 0.6602, 836.9069\n",
      "Training cce, ese, loss (for one batch): 695.3901, 0.7119, 696.1021\n",
      "Training cce, ese, loss (for one batch): 445.8279, 0.5489, 446.3769\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            loss_value = loss_fn(y_batch_train, logits, verbose =(step % 200 == 0)) #verbose every 200 batches\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
