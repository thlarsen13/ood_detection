{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow_probability.python import math as tfp_math\n",
    "from tensorflow_probability.python.internal import dtype_util\n",
    "from tensorflow_probability.python.internal import prefer_static as ps\n",
    "from tensorflow_probability.python.internal import tensorshape_util\n",
    "from tensorflow_probability.python.stats import quantiles as quantiles_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "cal_optimizer = keras.optimizers.SGD(learning_rate=1e-1)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_calibration_bin_statistics(\n",
    "    num_bins, logits=None, labels_true=None):\n",
    "    \"\"\" Compute binning statistics required for calibration measures.\n",
    "  Args:\n",
    "    num_bins: int, number of probability bins, e.g. 10.\n",
    "    logits: Tensor, (n,nlabels), with logits for n instances and nlabels.\n",
    "    labels_true: Tensor, (n,), with tf.int32 or tf.int64 elements containing\n",
    "      ground truth class labels in the range [0,nlabels].\n",
    "  Returns:\n",
    "    bz: Tensor, shape (2,num_bins), tf.int32, counts of incorrect (row 0) and\n",
    "      correct (row 1) predictions in each of the `num_bins` probability bins.\n",
    "    pmean_observed: Tensor, shape (num_bins,), tf.float32, the mean predictive\n",
    "      probabilities in each probability bin.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We take the label with the maximum probability\n",
    "    # decision.  This corresponds to the optimal expected minimum loss decision\n",
    "    # under 0/1 loss.\n",
    "#         pred_y = tf.argmax(logits, axis=1, output_type=labels_true.dtype)\n",
    "    pred_y = tf.argmax(logits, axis=1)\n",
    "\n",
    "\n",
    "    correct = tf.cast(tf.equal(pred_y, labels_true), tf.int32)\n",
    "\n",
    "    # Collect predicted probabilities of decisions\n",
    "    pred = tf.nn.softmax(logits, axis=1)\n",
    "    prob_y = tf.gather(\n",
    "      pred, pred_y[:, tf.newaxis], batch_dims=1)  # p(pred_y | x)\n",
    "    prob_y = tf.reshape(prob_y, (ps.size(prob_y),))\n",
    "\n",
    "    # Compute b/z histogram statistics:\n",
    "    # bz[0,bin] contains counts of incorrect predictions in the probability bin.\n",
    "    # bz[1,bin] contains counts of correct predictions in the probability bin.\n",
    "    bins = tf.histogram_fixed_width_bins(prob_y, [0.0, 1.0], nbins=num_bins)\n",
    "    event_bin_counts = tf.math.bincount(\n",
    "      correct * num_bins + bins,\n",
    "      minlength=2 * num_bins,\n",
    "      maxlength=2 * num_bins)\n",
    "    event_bin_counts = tf.reshape(event_bin_counts, (2, num_bins))\n",
    "\n",
    "    # Compute mean predicted probability value in each of the `num_bins` bins\n",
    "    pmean_observed = tf.math.unsorted_segment_sum(prob_y, bins, num_bins)\n",
    "    tiny = np.finfo(dtype_util.as_numpy_dtype(logits.dtype)).tiny\n",
    "    pmean_observed = pmean_observed / (\n",
    "      tf.cast(tf.reduce_sum(event_bin_counts, axis=0), logits.dtype) + tiny)\n",
    "\n",
    "    return event_bin_counts, pmean_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeESE(num_bins, logits=None, labels_true=None): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_bins: int, number of probability bins to compute oberved likelyhoods of correctness, e.g. 10.\n",
    "        logits: Tensor, (n,nlabels), with logits for n instances and nlabels.\n",
    "        labels_true: Tensor, (n,), with tf.int32 or tf.int64 elements containing\n",
    "          ground truth class labels in the range [0,nlabels].\n",
    "    Returns:\n",
    "        ece: Tensor, scalar, tf.float32.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('expected_calibration_error'):\n",
    "        logits = tf.convert_to_tensor(logits)\n",
    "        labels_true = tf.convert_to_tensor(labels_true)\n",
    "        labels_true = tf.cast(labels_true, dtype=tf.int64)\n",
    "\n",
    "        # Compute empirical counts over the events defined by the sets\n",
    "        # {incorrect,correct}x{0,1,..,num_bins-1}, as well as the empirical averages\n",
    "        # of predicted probabilities in each probability bin.\n",
    "        event_bin_counts, pmean_observed = _compute_calibration_bin_statistics(\n",
    "            num_bins, logits=logits, labels_true=labels_true)\n",
    "\n",
    "        # Compute the marginal probability of observing a probability bin.\n",
    "        event_bin_counts = tf.cast(event_bin_counts, tf.float32)\n",
    "        bin_n = tf.reduce_sum(event_bin_counts, axis=0)\n",
    "        pbins = bin_n / tf.reduce_sum(bin_n)  # Compute the marginal bin probability\n",
    "\n",
    "        # Compute the marginal probability of making a correct decision given an\n",
    "        # observed probability bin.\n",
    "        tiny = np.finfo(np.float32).tiny\n",
    "        pcorrect = event_bin_counts[1, :] / (bin_n + tiny)\n",
    "\n",
    "        # Compute the ESE statistic which is supposed to be analagous to brier score\n",
    "        ese = tf.reduce_sum(pbins * tf.square(pcorrect - pmean_observed))\n",
    "    return ese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Training loss (for one batch) at step 0: 0.1651\n",
      "Training ESE (for one batch) at step 0: 0.0141\n",
      "Training loss (for one batch) at step 200: 2.0642\n",
      "Training ESE (for one batch) at step 200: 0.0010\n",
      "Training loss (for one batch) at step 400: 3.0357\n",
      "Training ESE (for one batch) at step 400: 0.0098\n",
      "Training loss (for one batch) at step 600: 1.7804\n",
      "Training ESE (for one batch) at step 600: 0.0002\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.2923\n",
      "Training ESE (for one batch) at step 0: 0.0097\n",
      "Training loss (for one batch) at step 200: 1.4503\n",
      "Training ESE (for one batch) at step 200: 0.0157\n",
      "Training loss (for one batch) at step 400: 9.7349\n",
      "Training ESE (for one batch) at step 400: 0.0550\n",
      "Training loss (for one batch) at step 600: 1.6969\n",
      "Training ESE (for one batch) at step 600: 0.0064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Keep track of observed correct probabilities in each bin. Loss = \\sum_i (obs_bin_i - p_hat) ^2 (Brier)\n",
    "            # Key point: avoid discontinuity by holding fixed which bin each sample geos to\n",
    "            # p_hat is the softmax probability coming out of model \n",
    "            num_bins=10\n",
    "            ese_loss = computeESE(num_bins, logits=logits, labels_true=y_batch_train)\n",
    "            ese_grads = tape.gradient(ese_loss, model.trainable_weights)\n",
    "            cal_optimizer.apply_gradients(zip(ese_grads, model.trainable_weights))\n",
    "            \n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss_value))\n",
    "                )\n",
    "                print(\n",
    "                    \"Training ESE (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(ese_loss))\n",
    "                )\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
